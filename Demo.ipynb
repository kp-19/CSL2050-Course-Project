{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\91911\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Importing Necessary Libraries:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import wordcloud\n",
    "import warnings\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import sparse\n",
    "%matplotlib inline\n",
    "nltk.download('stopwords')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Trained_Models\\\\Decision_tree_tfid.joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8436\\3156957754.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Importing Models:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mjoblib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdt_clf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Trained_Models\\Decision_tree_tfid.joblib\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mrf_clf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Trianed_Models\\Random_forest_tfid.joblib\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnb_clf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Trained_Models\\ComplimentNB_tfid.joblib\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\91911\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    577\u001b[0m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    580\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0m_read_fileobject\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfobj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Trained_Models\\\\Decision_tree_tfid.joblib'"
     ]
    }
   ],
   "source": [
    "#Importing Models:\n",
    "from joblib import load\n",
    "dt_clf = load(filename=\"webapp/Trained_models/Decision_tree_tfid.joblib\")\n",
    "rf_clf = load(filename=\"webapp/Trained_models/Random_forest_tfid_100_trees.joblib\")\n",
    "nb_clf = load(filename=\"webapp/Trained_models/ComplimentNB_tfid.joblib\")\n",
    "svm_clf = load(filename=\"webapp/Trained_models/SVM_linear_kernel_tfid.joblib\")\n",
    "perc_clf = load(filename=\"webapp/Trained_models/Perceptron_tfid.joblib\")\n",
    "lr_clf = load(filename=\"webapp/Trained_models/Logistic_regression_tfid.joblib\")\n",
    "vectorizer = load(filename=\"webapp/Trained_models/Vectorizer.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "def stem_text(txt_input):\n",
    "\n",
    "    #Apply stemming to input text:\n",
    "    stemmer = PorterStemmer()\n",
    "    txt_series = pd.Series(txt_input)\n",
    "    stemmed_txt = txt_series.apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))\n",
    "\n",
    "    return stemmed_txt\n",
    "\n",
    "def preprocess_input(text, age, time_of_tweet):\n",
    "\n",
    "    processed_text = text.replace(\"[^a-zA-Z#]\", \" \")\n",
    "    processed_text = processed_text.lower()\n",
    "    \n",
    "    # Apply Porter stemming\n",
    "    processed_text = stem_text(processed_text)\n",
    "    \n",
    "    # Create a DataFrame to hold the preprocessed data\n",
    "    processed_data = pd.DataFrame({'processed_text': processed_text, 'age': age, 'time_of_tweet': time_of_tweet})\n",
    "    \n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mode\n",
    "\n",
    "def ensemble_predict(X_test, dt_clf, rf_clf, nb_clf, svm_clf, perc_clf, lr_clf):\n",
    "    X_test_array = X_test.toarray()\n",
    "\n",
    "    # Initialize lists to store predictions from each classifier\n",
    "    dt_predictions = []\n",
    "    rf_predictions = []\n",
    "    svm_predictions = []\n",
    "    lr_predictions = []\n",
    "    nb_predictions = []\n",
    "    perc_predictions = []\n",
    "\n",
    "    # Getting the predictions:\n",
    "    for data_point in X_test_array:\n",
    "        dt_pred = dt_clf.predict(data_point.reshape(1, -1))\n",
    "        dt_predictions.append(dt_pred[0])    \n",
    "    rf_predictions = rf_clf.predict(X_test)\n",
    "    nb_predictions = nb_clf.predict(X_test)\n",
    "    perc_predictions = perc_clf.predict(X_test)\n",
    "    lr_predictions = lr_clf.predict(X_test)\n",
    "    svm_predictions = svm_clf.predict(X_test)\n",
    "    \n",
    "    # Initialize list to store final ensemble predictions\n",
    "    ensemble_predictions = []\n",
    "\n",
    "    # Combine predictions from all classifiers\n",
    "    for i in range(len(X_test_array)):\n",
    "        # Calculate mode label from predictions of all classifiers\n",
    "        mode_label = mode([dt_predictions[i], rf_predictions[i], lr_predictions[i], nb_predictions[i], perc_predictions[i], svm_predictions[i]])\n",
    "        ensemble_predictions.append(mode_label)\n",
    "\n",
    "    return ensemble_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label(text, age, time_of_tweet):\n",
    "\n",
    "    processed_data = preprocess_input(text, age, time_of_tweet)\n",
    "    \n",
    "    # Vectorize the processed text\n",
    "    vectorized_input = vectorizer.transform(processed_data['processed_text'])\n",
    "    \n",
    "    # Concatenate the age and time_of_tweet features with the vectorized text\n",
    "    X_input = sparse.hstack([processed_data[['age', 'time_of_tweet']], vectorized_input])\n",
    "    \n",
    "    predictions = ensemble_predict(X_input, dt_clf, rf_clf, nb_clf, svm_clf, perc_clf, lr_clf)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label:Positive\n"
     ]
    }
   ],
   "source": [
    "user_text = input(\"Enter your text: \")\n",
    "user_age = int(input(\"Enter your age: \"))\n",
    "user_time_of_tweet = int(input(\"Enter time of tweet (morning=0, noon=1, night=2): \"))\n",
    "\n",
    "predicted_label = predict_label(user_text, user_age, user_time_of_tweet)\n",
    "if (predicted_label[0]==1):\n",
    "    print(\"Predicted label:Neutral\")\n",
    "if (predicted_label[0]==2):\n",
    "    print(\"Predicted label:Positive\")\n",
    "if (predicted_label[0]==0):\n",
    "    print(\"Predicted label:Negative\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
